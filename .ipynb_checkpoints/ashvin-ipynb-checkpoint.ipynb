{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9785132,"sourceType":"datasetVersion","datasetId":5985849}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\nimport torch\n\nfrom transformers import BertTokenizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import BertModel\n\nimport torch.nn as nn\n\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport torch.optim as optim\n\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(torch.cuda.is_available())  # Should print True if CUDA is detected\n\nprint(torch.cuda.device_count())  # Should return the number of GPUs available\n\nprint(torch.cuda.get_device_name(0))  # Should print the name of the GPU (e.g., NVIDIA GeForce RTX 4060)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:56:02.513476Z","iopub.execute_input":"2024-11-02T16:56:02.513872Z","iopub.status.idle":"2024-11-02T16:56:08.690273Z","shell.execute_reply.started":"2024-11-02T16:56:02.513832Z","shell.execute_reply":"2024-11-02T16:56:08.689337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your dataset\n\ndata = pd.read_csv(\"/kaggle/input/cyberguard/Modified_Data.csv\")\n\n\n\n# Split into train and validation sets\n\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\nfrom transformers import ElectraTokenizer\n\n# Load the ELECTRA tokenizer\ntokenizer = ElectraTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:56:08.691947Z","iopub.execute_input":"2024-11-02T16:56:08.692416Z","iopub.status.idle":"2024-11-02T16:56:10.136985Z","shell.execute_reply.started":"2024-11-02T16:56:08.692382Z","shell.execute_reply":"2024-11-02T16:56:10.136191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize data\n\ndef tokenize_data(data):\n\n    return tokenizer(\n\n        data[\"crimeaditionalinfo\"].tolist(),\n\n        padding=True,\n\n        truncation=True,\n\n        max_length=128,\n\n        return_tensors=\"pt\"\n\n    )\n\n\n\n# Tokenize both train and validation data\n\ntrain_encodings = tokenize_data(train_data)\n\nval_encodings = tokenize_data(val_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T16:56:15.394651Z","iopub.execute_input":"2024-11-02T16:56:15.395538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert labels to tensors\n\ntrain_labels_category = torch.tensor(train_data[\"category\"].values)\n\ntrain_labels_sub_category = torch.tensor(train_data[\"sub_category\"].values)\n\n\n\nval_labels_category = torch.tensor(val_data[\"category\"].values)\n\nval_labels_sub_category = torch.tensor(val_data[\"sub_category\"].values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert labels to long data type\n\ntrain_labels_category = train_labels_category.long()\n\ntrain_labels_sub_category = train_labels_sub_category.long()\n\n\n\nval_labels_category = val_labels_category.long()\n\nval_labels_sub_category = val_labels_sub_category.long()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create datasets and data loaders\n\ntrain_dataset = TensorDataset(\n\n    train_encodings['input_ids'], train_encodings['attention_mask'],\n\n    train_labels_category, train_labels_sub_category\n\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n\n\n# Similar setup for the validation loader\n\nval_dataset = TensorDataset(\n\n    val_encodings['input_ids'], val_encodings['attention_mask'],\n\n    val_labels_category, val_labels_sub_category\n\n)\n\nval_loader = DataLoader(val_dataset, batch_size=8)\nfrom transformers import XLNetModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import ElectraModel\n\nclass MultiTaskModel(nn.Module):\n    def __init__(self, model_name, num_labels_task1, num_labels_task2):\n        super(MultiTaskModel, self).__init__()\n        self.electra = ElectraModel.from_pretrained(model_name)\n        self.classifier_task1 = nn.Linear(self.electra.config.hidden_size, num_labels_task1)\n        self.classifier_task2 = nn.Linear(self.electra.config.hidden_size, num_labels_task2)\n\n    def forward(self, input_ids, attention_mask, labels_task1=None, labels_task2=None):\n        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n        # Use the first token (typically the [CLS] token) for classification\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        \n        logits_task1 = self.classifier_task1(pooled_output)\n        logits_task2 = self.classifier_task2(pooled_output)\n\n        loss = None\n        if labels_task1 is not None and labels_task2 is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            loss_task1 = loss_fn(logits_task1, labels_task1)\n            loss_task2 = loss_fn(logits_task2, labels_task2)\n            loss = loss_task1 + loss_task2\n\n        return {\"loss\": loss, \"logits_task1\": logits_task1, \"logits_task2\": logits_task2}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_labels_task1 = 15\n\nnum_labels_task2 = 35\n\nfrom transformers import XLNetTokenizer, XLNetForSequenceClassification\n\n\nprint(f\"Unique classes in 'category': {num_labels_task1}\")\n\nprint(f\"Unique classes in 'sub_category': {num_labels_task2}\")\n\n\n\n# Initialize the model with dynamic class counts\n\nmodel = MultiTaskModel(\"google/electra-base-discriminator\", num_labels_task1=num_labels_task1, num_labels_task2=num_labels_task2)\n\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create data loaders\n\ntrain_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'],\n\n                              train_labels_category, train_labels_sub_category)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n\n\n# Optimizer\n\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize lists to store metrics\n\ntrain_accuracies_task1 = []\n\ntrain_accuracies_task2 = []\n\nval_accuracies_task1 = []\n\nval_accuracies_task2 = []","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example Training Loop with Accuracy Tracking\n\nfor epoch in range(5):\n\n    model.train()\n\n    total_train_loss = 0\n\n    train_preds_task1, train_preds_task2 = [], []\n\n    train_labels_task1, train_labels_task2 = [], []\n\n\n\n    for batch in train_loader:\n\n        # Move batch data to the device\n\n        input_ids = batch[0].to(device)\n\n        attention_mask = batch[1].to(device)\n\n        labels_task1 = batch[2].to(device)\n\n        labels_task2 = batch[3].to(device)\n\n\n\n        # Forward pass\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n\n                        labels_task1=labels_task1, labels_task2=labels_task2)\n\n\n\n        loss = outputs[\"loss\"]\n\n        total_train_loss += loss.item()\n\n\n\n        # Backpropagation\n\n        loss.backward()\n\n        optimizer.step()\n\n        optimizer.zero_grad()\n\n\n\n        # Move predictions and labels back to CPU for accuracy calculation\n\n        train_preds_task1.extend(torch.argmax(outputs[\"logits_task1\"], axis=1).cpu().numpy())\n\n        train_preds_task2.extend(torch.argmax(outputs[\"logits_task2\"], axis=1).cpu().numpy())\n\n        train_labels_task1.extend(labels_task1.cpu().numpy())\n\n        train_labels_task2.extend(labels_task2.cpu().numpy())\n\n\n\n    # Calculate training accuracy\n\n    train_accuracy_task1 = accuracy_score(train_labels_task1, train_preds_task1)\n\n    train_accuracy_task2 = accuracy_score(train_labels_task2, train_preds_task2)\n\n    train_accuracies_task1.append(train_accuracy_task1)\n\n    train_accuracies_task2.append(train_accuracy_task2)\n\n\n\n    # Validation phase\n\n    model.eval()\n\n    val_preds_task1, val_preds_task2 = [], []\n\n    val_labels_task1, val_labels_task2 = [], []\n\n    with torch.no_grad():\n\n        for batch in val_loader:\n\n            # Move batch data to the device\n\n            input_ids = batch[0].to(device)\n\n            attention_mask = batch[1].to(device)\n\n            labels_task1 = batch[2].to(device)\n\n            labels_task2 = batch[3].to(device)\n\n\n\n            # Forward pass\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n\n\n            # Move predictions and labels back to CPU for accuracy calculation\n\n            val_preds_task1.extend(torch.argmax(outputs[\"logits_task1\"], axis=1).cpu().numpy())\n\n            val_preds_task2.extend(torch.argmax(outputs[\"logits_task2\"], axis=1).cpu().numpy())\n\n            val_labels_task1.extend(labels_task1.cpu().numpy())\n\n            val_labels_task2.extend(labels_task2.cpu().numpy())\n\n\n\n    # Calculate validation accuracy\n\n    val_accuracy_task1 = accuracy_score(val_labels_task1, val_preds_task1)\n\n    val_accuracy_task2 = accuracy_score(val_labels_task2, val_preds_task2)\n\n    val_accuracies_task1.append(val_accuracy_task1)\n\n    val_accuracies_task2.append(val_accuracy_task2)\n\n\n\n    print(f\"Epoch {epoch+1} - Training Loss: {total_train_loss / len(train_loader)}\")\n\n    print(f\"Task 1 - Training Accuracy: {train_accuracy_task1}, Validation Accuracy: {val_accuracy_task1}\")\n\n    print(f\"Task 2 - Training Accuracy: {train_accuracy_task2}, Validation Accuracy: {val_accuracy_task2}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Training and Validation Accuracy for Task 1\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\n\nplt.plot(train_accuracies_task1, label='Train Accuracy - Task 1')\n\nplt.plot(val_accuracies_task1, label='Val Accuracy - Task 1')\n\nplt.xlabel('Epochs')\n\nplt.ylabel('Accuracy')\n\nplt.title('Task 1: Category Classification')\n\nplt.legend()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Training and Validation Accuracy for Task 2\n\nplt.subplot(1, 2, 2)\n\nplt.plot(train_accuracies_task2, label='Train Accuracy - Task 2')\n\nplt.plot(val_accuracies_task2, label='Val Accuracy - Task 2')\n\nplt.xlabel('Epochs')\n\nplt.ylabel('Accuracy')\n\nplt.title('Task 2: Sub-Category Classification')\n\nplt.legend()\n\n\n\nplt.tight_layout()\n\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null}]}